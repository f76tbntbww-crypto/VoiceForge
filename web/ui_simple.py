# -*- coding: utf-8 -*-
"""
VoiceForge Web UI - ç®€åŒ–ç‰ˆ

åŸºäº Gradio çš„ Web ç•Œé¢
æä¾›ï¼šè¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆã€AIå¯¹è¯åŠŸèƒ½ï¼ˆæ”¯æŒå›¾ç‰‡ä¸Šä¼ å’Œå¤šè½®è®°å¿†ï¼‰

å¯åŠ¨æ–¹å¼ï¼š
    python web/ui_simple.py

æˆ–ä½¿ç”¨è„šæœ¬ï¼š
    ..\scripts\start_web.bat
"""

import os
import sys
import uuid
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import gradio as gr

# å¯¼å…¥æ’ä»¶
from plugins.asr.sensevoice import SenseVoiceASR
from plugins.tts.cosyvoice import CosyVoiceTTS

# ==================== é…ç½®ç®¡ç† ====================


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - æ”¯æŒçƒ­æ›´æ–°"""

    def __init__(self, config_path: Path = None):
        self.config_path = config_path or project_root / "config.yaml"
        self._config = None
        self._load()

    def _load(self):
        """åŠ è½½é…ç½®"""
        with open(self.config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # å¤„ç†ç›¸å¯¹è·¯å¾„
        paths = config.get("paths", {})
        if "models" in paths:
            for key, path in paths["models"].items():
                if not os.path.isabs(path):
                    paths["models"][key] = os.path.join(project_root, path)
        if "libs" in paths:
            for key, path in paths["libs"].items():
                if not os.path.isabs(path):
                    paths["libs"][key] = os.path.join(project_root, path)
                if path not in sys.path:
                    sys.path.insert(0, paths["libs"][key])

        self._config = config

    def save(self):
        """ä¿å­˜é…ç½®"""
        with open(self.config_path, "w", encoding="utf-8") as f:
            yaml.dump(self._config, f, allow_unicode=True, sort_keys=False)

    @property
    def config(self) -> dict:
        """è·å–å½“å‰é…ç½®ï¼ˆå®æ—¶ï¼‰"""
        return self._config

    def get(self, path: str, default=None):
        """é€šè¿‡è·¯å¾„è·å–é…ç½®å€¼ï¼Œä¾‹å¦‚ï¼šmodels.llm.ollama.max_tokens"""
        keys = path.split(".")
        value = self._config
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        return value

    def set(self, path: str, value):
        """é€šè¿‡è·¯å¾„è®¾ç½®é…ç½®å€¼"""
        keys = path.split(".")
        config = self._config
        for key in keys[:-1]:
            if key not in config:
                config[key] = {}
            config = config[key]
        config[keys[-1]] = value


# å…¨å±€é…ç½®ç®¡ç†å™¨
config_manager = ConfigManager()


# ==================== è®°å¿†ç®¡ç†å™¨ ====================


class ChatMemory:
    """
    èŠå¤©è®°å¿†ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    - ä¿å­˜å¯¹è¯å†å²
    - æ”¯æŒæ»‘åŠ¨çª—å£ï¼ˆé™åˆ¶è½®æ•°ï¼‰
    - æŒ‰ä¼šè¯IDéš”ç¦»ä¸åŒå¯¹è¯
    - æ”¯æŒæ¸…ç©ºè®°å¿†
    """

    def __init__(self, max_history: int = 10):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            max_history: æœ€å¤§ä¿å­˜çš„å¯¹è¯è½®æ•°ï¼ˆç”¨æˆ·+åŠ©æ‰‹ç®—ä¸€è½®ï¼‰
        """
        self.max_history = max_history
        self._sessions: Dict[str, List[Dict[str, Any]]] = {}

    def create_session(self) -> str:
        """åˆ›å»ºæ–°ä¼šè¯ï¼Œè¿”å›ä¼šè¯ID"""
        session_id = str(uuid.uuid4())[:8]  # ä½¿ç”¨çŸ­UUID
        self._sessions[session_id] = []
        return session_id

    def add(self, session_id: str, role: str, content: str, image: str = None):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°è®°å¿†

        Args:
            session_id: ä¼šè¯ID
            role: è§’è‰² ("user" æˆ– "assistant")
            content: æ¶ˆæ¯å†…å®¹
            image: å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        if session_id not in self._sessions:
            self._sessions[session_id] = []

        message = {"role": role, "content": content}
        if image:
            message["image"] = image

        self._sessions[session_id].append(message)

        # æ»‘åŠ¨çª—å£ï¼šä¿ç•™æœ€è¿‘ N è½®å¯¹è¯ï¼ˆç”¨æˆ·+åŠ©æ‰‹=ä¸€è½®ï¼‰
        # æ¯è½®2æ¡æ¶ˆæ¯ï¼Œä¿ç•™ max_history * 2 æ¡
        max_messages = self.max_history * 2
        if len(self._sessions[session_id]) > max_messages:
            # ä¿ç•™æœ€æ–°çš„æ¶ˆæ¯
            self._sessions[session_id] = self._sessions[session_id][-max_messages:]

    def get(self, session_id: str, include_system: bool = True) -> List[Dict[str, Any]]:
        """
        è·å–ä¼šè¯å†å²

        Args:
            session_id: ä¼šè¯ID
            include_system: æ˜¯å¦åŒ…å« System Message

        Returns:
            æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äº API è°ƒç”¨ï¼‰
        """
        messages = []

        # æ·»åŠ  System Message
        if include_system:
            system_prompt = config_manager.get(
                "models.llm.ollama.system_prompt",
                "ä½ å¿…é¡»åœ¨é™å®šå­—æ•°å†…å®Œæ•´è¡¨è¾¾ã€‚å¦‚æœå†…å®¹è¾ƒé•¿ï¼Œè¯·ç²¾ç®€å›ç­”ï¼Œç¡®ä¿ç»“å°¾å®Œæ•´ã€æ„æ€æ¸…æ™°ã€‚ä¸è¦è¯´åˆ°ä¸€åŠå°±åœæ­¢ã€‚ä¼˜å…ˆç»™å‡ºæ ¸å¿ƒç»“è®ºï¼Œç»†èŠ‚å¯çœç•¥ã€‚",
            )
            messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ å†å²å¯¹è¯
        if session_id in self._sessions:
            for msg in self._sessions[session_id]:
                api_msg = {"role": msg["role"], "content": msg["content"]}
                # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡ä¿¡æ¯ï¼ˆç”¨äºå¤šæ¨¡æ€æ¨¡å‹ï¼‰
                if "image" in msg and msg["image"]:
                    # è¯»å–å›¾ç‰‡å¹¶è½¬ä¸ºbase64
                    try:
                        import base64

                        with open(msg["image"], "rb") as f:
                            img_base64 = base64.b64encode(f.read()).decode("utf-8")
                        api_msg["images"] = [img_base64]
                    except:
                        pass
                messages.append(api_msg)

        return messages

    def get_display_history(self, session_id: str) -> List:
        """
        è·å–ç”¨äºæ˜¾ç¤ºçš„å†å²è®°å½•ï¼ˆGradio Chatbot æ ¼å¼ï¼‰

        Returns:
            Gradio Chatbot æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        history = []
        if session_id not in self._sessions:
            return history

        for msg in self._sessions[session_id]:
            if msg["role"] == "user":
                # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨Gradio Chatbotæ”¯æŒçš„å›¾ç‰‡æ ¼å¼
                if "image" in msg and msg["image"] and os.path.exists(msg["image"]):
                    # Gradio Chatbotæ”¯æŒåœ¨contentä¸­åŒ…å«å›¾ç‰‡è·¯å¾„
                    if msg["content"] and msg["content"].strip():
                        # æœ‰æ–‡å­—ä¹Ÿæœ‰å›¾ç‰‡
                        history.append(
                            {
                                "role": "user",
                                "content": {
                                    "path": msg["image"],
                                    "text": msg["content"],
                                },
                            }
                        )
                    else:
                        # åªæœ‰å›¾ç‰‡æ²¡æœ‰æ–‡å­—
                        history.append(
                            {"role": "user", "content": {"path": msg["image"]}}
                        )
                else:
                    # çº¯æ–‡å­—
                    history.append({"role": "user", "content": msg["content"]})
            elif msg["role"] == "assistant":
                history.append({"role": "assistant", "content": msg["content"]})

        return history

    def clear(self, session_id: str):
        """æ¸…ç©ºæŒ‡å®šä¼šè¯çš„è®°å¿†"""
        if session_id in self._sessions:
            self._sessions[session_id] = []

    def get_session_count(self) -> int:
        """è·å–ä¼šè¯æ•°é‡"""
        return len(self._sessions)

    def get_message_count(self, session_id: str) -> int:
        """è·å–æŒ‡å®šä¼šè¯çš„æ¶ˆæ¯æ•°é‡"""
        return len(self._sessions.get(session_id, []))


# å…¨å±€è®°å¿†ç®¡ç†å™¨
chat_memory = ChatMemory(max_history=10)


# ==================== åŠ è½½é…ç½® ====================

config = config_manager.config
web_config = config.get("web", {}).get("simple", {})
models_config = config.get("models", {})
paths_config = config.get("paths", {})

# ==================== åŠ è½½æ¨¡å‹ ====================

print("=" * 60)
print("ğŸš€ VoiceForge Web UI (ç®€åŒ–ç‰ˆ)")
print("=" * 60)

# åŠ è½½ ASR
asr_model = None
if models_config.get("asr", {}).get("enabled", True):
    print("\nğŸ”„ åŠ è½½ ASR æ¨¡å‹ | Loading ASR Model...")
    asr_config = models_config["asr"].copy()
    asr_config["model_path"] = paths_config.get("models", {}).get("asr")
    asr_model = SenseVoiceASR(asr_config)
    asr_model.load(asr_config)

# åŠ è½½ TTS
tts_model = None
if models_config.get("tts", {}).get("enabled", True):
    print("\nğŸ”„ åŠ è½½ TTS æ¨¡å‹ | Loading TTS Model...")
    tts_config = models_config["tts"].copy()
    tts_config["model_path"] = paths_config.get("models", {}).get("tts")
    tts_config["cosyvoice_lib"] = paths_config.get("libs", {}).get("cosyvoice")
    tts_model = CosyVoiceTTS(tts_config)
    tts_model.load(tts_config)

# è·å–éŸ³è‰²åˆ—è¡¨
voices = ["ä¸­æ–‡å¥³", "ä¸­æ–‡ç”·", "æ—¥è¯­ç”·", "ç²¤è¯­å¥³", "è‹±æ–‡å¥³", "è‹±æ–‡ç”·", "éŸ©è¯­å¥³"]
if tts_model and tts_model.is_loaded():
    try:
        voice_list = tts_model.get_voices()
        voices = [v["name"] for v in voice_list]
    except:
        pass

print("\n" + "=" * 60)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ | Models Loaded")
print(
    f"ğŸ“ è®°å¿†ç®¡ç†å™¨å·²å¯åŠ¨ | Memory Manager Startedï¼ˆæœ€å¤§ä¿ç•™ | Max {chat_memory.max_history} è½®å¯¹è¯ | roundsï¼‰"
)
print("=" * 60)

# ==================== åŠŸèƒ½å‡½æ•° ====================


def speech_to_text(audio_file, language):
    """è¯­éŸ³è¯†åˆ« | Speech Recognition"""
    if not asr_model or not asr_model.is_loaded():
        return "é”™è¯¯ | Error: ASRæ¨¡å‹æœªåŠ è½½ | ASR model not loaded", ""

    if audio_file is None:
        return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ | Please upload audio file first", ""

    try:
        result = asr_model.transcribe(audio_file, language)
        if result.get("success"):
            return result["text"], f"è¯­è¨€ | Language: {result['language']}"
        else:
            return f"è¯†åˆ«å¤±è´¥ | Recognition failed: {result.get('error', '')}", ""
    except Exception as e:
        return f"é”™è¯¯ | Error: {str(e)}", ""


def text_to_speech(text, voice):
    """è¯­éŸ³åˆæˆ | Text to Speech"""
    if not tts_model or not tts_model.is_loaded():
        return None, "é”™è¯¯ | Error: TTSæ¨¡å‹æœªåŠ è½½ | TTS model not loaded"

    if not text.strip():
        return None, "è¯·è¾“å…¥æ–‡æœ¬ | Please enter text"

    try:
        audio_path = tts_model.synthesize(text, voice)
        return audio_path, "åˆæˆæˆåŠŸ | Synthesis successful"
    except Exception as e:
        return None, f"é”™è¯¯ | Error: {str(e)}"


def chat_with_ai(session_id: str, message: str, image: str = None) -> str:
    """
    AIå¯¹è¯ï¼ˆæ”¯æŒå›¾ç‰‡å’Œå¤šè½®è®°å¿†ï¼‰| AI Chat (supports image and multi-turn memory)

    Args:
        session_id: ä¼šè¯ID | Session ID
        message: ç”¨æˆ·æ¶ˆæ¯ | User message
        image: å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰| Image path (optional)

    Returns:
        AIå›å¤å†…å®¹ | AI response content
    """
    if not models_config.get("llm", {}).get("enabled", True):
        return "é”™è¯¯ | Error: LLMå·²ç¦ç”¨ | LLM disabled"

    if not message.strip() and image is None:
        return "è¯·è¾“å…¥æ¶ˆæ¯æˆ–ä¸Šä¼ å›¾ç‰‡ | Please enter message or upload image"

    import requests

    # è·å–å®æ—¶é…ç½®ï¼ˆçƒ­æ›´æ–°ï¼‰
    max_tokens = config_manager.get("models.llm.ollama.max_tokens", 80)
    system_prompt = config_manager.get(
        "models.llm.ollama.system_prompt",
        "ä½ å¿…é¡»åœ¨é™å®šå­—æ•°å†…å®Œæ•´è¡¨è¾¾ã€‚å¦‚æœå†…å®¹è¾ƒé•¿ï¼Œè¯·ç²¾ç®€å›ç­”ï¼Œç¡®ä¿ç»“å°¾å®Œæ•´ã€æ„æ€æ¸…æ™°ã€‚ä¸è¦è¯´åˆ°ä¸€åŠå°±åœæ­¢ã€‚ä¼˜å…ˆç»™å‡ºæ ¸å¿ƒç»“è®ºï¼Œç»†èŠ‚å¯çœç•¥ã€‚",
    )

    try:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿†
        chat_memory.add(session_id, "user", message, image)

        # è·å–åŒ…å«å†å²çš„å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        messages = chat_memory.get(session_id, include_system=True)

        payload = {
            "model": config_manager.get("models.llm.ollama.model", "gemma3:4b"),
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": config_manager.get("models.llm.ollama.temperature", 0.7),
                "num_predict": max_tokens,
            },
        }

        response = requests.post(
            f"{config_manager.get('models.llm.ollama.url', 'http://localhost:11434')}/api/chat",
            json=payload,
            timeout=60,
        )

        if response.status_code == 200:
            data = response.json()
            ai_response = data.get("message", {}).get("content", "æ— å›å¤ | No response")

            # æ·»åŠ AIå›å¤åˆ°è®°å¿†
            chat_memory.add(session_id, "assistant", ai_response)

            return ai_response
        else:
            return f"è°ƒç”¨å¤±è´¥ | Request failed: HTTP {response.status_code}"
    except Exception as e:
        return f"é”™è¯¯ | Error: {str(e)}"


def update_settings(max_tokens_input, system_prompt_input, max_history_input):
    """æ›´æ–°è®¾ç½®å¹¶ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ï¼ˆçƒ­æ›´æ–°ï¼‰| Update settings and save to config file (hot-reload)"""
    try:
        # æ›´æ–°é…ç½® | Update configuration
        config_manager.set("models.llm.ollama.max_tokens", int(max_tokens_input))
        config_manager.set("models.llm.ollama.system_prompt", system_prompt_input)

        # æ›´æ–°è®°å¿†ç®¡ç†å™¨çš„æœ€å¤§å†å²è½®æ•°
        global chat_memory
        chat_memory.max_history = int(max_history_input)

        # ä¿å­˜åˆ°æ–‡ä»¶ | Save to file
        config_manager.save()

        return (
            f"âœ… è®¾ç½®å·²æ›´æ–° | Settings Updated!\n"
            f"   â€¢ Tokené™åˆ¶ | Token Limit: {max_tokens_input}\n"
            f"   â€¢ æœ€å¤§è®°å¿†è½®æ•° | Max Memory Rounds: {max_history_input}\n"
            f"   â€¢ é…ç½®å·²ä¿å­˜å¹¶ç«‹å³ç”Ÿæ•ˆ | Config saved and active immediately"
        )
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥ | Save failed: {str(e)}"


def get_current_settings():
    """è·å–å½“å‰è®¾ç½®å€¼ï¼ˆå®æ—¶è¯»å–ï¼‰| Get current settings (real-time read)"""
    max_tokens = config_manager.get("models.llm.ollama.max_tokens", 80)
    system_prompt = config_manager.get(
        "models.llm.ollama.system_prompt",
        "ä½ å¿…é¡»åœ¨é™å®šå­—æ•°å†…å®Œæ•´è¡¨è¾¾ã€‚å¦‚æœå†…å®¹è¾ƒé•¿ï¼Œè¯·ç²¾ç®€å›ç­”ï¼Œç¡®ä¿ç»“å°¾å®Œæ•´ã€æ„æ€æ¸…æ™°ã€‚ä¸è¦è¯´åˆ°ä¸€åŠå°±åœæ­¢ã€‚ä¼˜å…ˆç»™å‡ºæ ¸å¿ƒç»“è®ºï¼Œç»†èŠ‚å¯çœç•¥ã€‚",
    )
    max_history = chat_memory.max_history
    return max_tokens, system_prompt, max_history


def clear_memory(session_id: str):
    """æ¸…ç©ºè®°å¿† | Clear Memory"""
    chat_memory.clear(session_id)
    message_count = chat_memory.get_message_count(session_id)
    return (
        f"ğŸ—‘ï¸ è®°å¿†å·²æ¸…ç©º | Memory Clearedï¼ˆå½“å‰ä¼šè¯æ¶ˆæ¯æ•° | Current session messages: {message_count}ï¼‰",
        [],
    )


def get_memory_info(session_id: str):
    """è·å–è®°å¿†ä¿¡æ¯ | Get Memory Info"""
    count = chat_memory.get_message_count(session_id)
    rounds = count // 2  # æ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹ä¸¤æ¡æ¶ˆæ¯
    max_rounds = chat_memory.max_history
    return f"ğŸ’¬ å½“å‰å¯¹è¯ | Current Chat: {rounds}/{max_rounds} è½® | rounds ({count} æ¡æ¶ˆæ¯ | messages)"


def complete_pipeline(session_id: str, audio_file, text_input, image_file, voice):
    """å®Œæ•´æµç¨‹ï¼šè¯­éŸ³/æ–‡å­—+å›¾ç‰‡ â†’ AIå›å¤ â†’ è¯­éŸ³æ’­æ”¾ | Full Pipeline: Voice/Text+Image â†’ AI â†’ Voice"""
    # ä¼˜å…ˆä½¿ç”¨è¯­éŸ³è¾“å…¥ï¼Œå¦‚æœæ²¡æœ‰è¯­éŸ³åˆ™ä½¿ç”¨æ–‡å­—è¾“å…¥
    if audio_file is None and not text_input.strip():
        return (
            None,
            "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–è¾“å…¥æ–‡å­— | Please upload audio or enter text",
            "",
        )

    try:
        # Step 1: è·å–è¾“å…¥ï¼ˆASRæˆ–ç›´æ¥ä½¿ç”¨æ–‡å­—ï¼‰| Get input (ASR or text)
        if audio_file is not None:
            # ä½¿ç”¨è¯­éŸ³è¯†åˆ« | Use speech recognition
            if not asr_model or not asr_model.is_loaded():
                return None, "é”™è¯¯ | Error: ASRæ¨¡å‹æœªåŠ è½½ | ASR model not loaded", ""

            asr_result = asr_model.transcribe(audio_file, "auto")
            if not asr_result.get("success"):
                return (
                    None,
                    f"è¯†åˆ«å¤±è´¥ | Recognition failed: {asr_result.get('error', '')}",
                    "",
                )
            recognized_text = asr_result["text"]
        else:
            # ç›´æ¥ä½¿ç”¨æ–‡å­—è¾“å…¥ | Use text input directly
            recognized_text = text_input.strip()

        # Step 2: LLMï¼ˆæ”¯æŒå›¾ç‰‡ï¼‰| LLM (supports image)
        if not models_config.get("llm", {}).get("enabled", True):
            return None, "é”™è¯¯ | Error: LLMå·²ç¦ç”¨ | LLM disabled", recognized_text

        ai_response = chat_with_ai(session_id, recognized_text, image_file)

        if ai_response.startswith("é”™è¯¯ï¼š") or ai_response.startswith("è°ƒç”¨å¤±è´¥ï¼š"):
            return (
                None,
                f"LLMè°ƒç”¨å¤±è´¥ | LLM request failed: {ai_response}",
                recognized_text,
            )

        # Step 3: TTS
        if not tts_model or not tts_model.is_loaded():
            return (
                None,
                "é”™è¯¯ | Error: TTSæ¨¡å‹æœªåŠ è½½ | TTS model not loaded",
                recognized_text,
            )

        audio_path = tts_model.synthesize(ai_response, voice)

        return audio_path, ai_response, recognized_text

    except Exception as e:
        return None, f"é”™è¯¯ | Error: {str(e)}", ""


# ==================== åˆ›å»ºç•Œé¢ ====================

# è·å–å½“å‰è®¾ç½®
current_max_tokens, current_system_prompt, current_max_history = get_current_settings()

# åˆå§‹åŒ–ä¼šè¯IDï¼ˆç”¨äºè®°å¿†ç®¡ç†ï¼‰
initial_session_id = chat_memory.create_session()

with gr.Blocks(title="VoiceForge | æœ¬åœ°AIè¯­éŸ³åŠ©æ‰‹") as demo:
    # éšè—çš„ä¼šè¯IDå­˜å‚¨
    session_id_state = gr.State(value=initial_session_id)

    gr.Markdown("""
    # ğŸ™ï¸ VoiceForge | æœ¬åœ°AIè¯­éŸ³åŠ©æ‰‹ | Local AI Voice Assistant
    
    **åŸºäº SenseVoice + CosyVoice + Ollama çš„å¼€æºè¯­éŸ³å¯¹è¯ç³»ç»Ÿ**
    
    **Open Source Voice Assistant powered by SenseVoice + CosyVoice + Ollama**
    
    å®Œå…¨æœ¬åœ°è¿è¡Œ | Fully Local  Â·  æ— éœ€è”ç½‘ | No Internet Required  Â·  ä¿æŠ¤éšç§ | Privacy Protected
    """)

    with gr.Tabs():
        # Tab 1: è¯­éŸ³è¯†åˆ« | Speech Recognition
        with gr.Tab("è¯­éŸ³è¯†åˆ« | Speech Recognition"):
            gr.Markdown("### ğŸ¤ è¯­éŸ³è¯†åˆ« | Speech Recognition")
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(
                        label="ä¸Šä¼ éŸ³é¢‘ | Upload Audio", type="filepath"
                    )
                    language = gr.Dropdown(
                        label="è¯­è¨€ | Language",
                        choices=[
                            ("è‡ªåŠ¨æ£€æµ‹ | Auto", "auto"),
                            ("ä¸­æ–‡ | Chinese", "zh"),
                            ("è‹±è¯­ | English", "en"),
                            ("æ—¥è¯­ | Japanese", "ja"),
                            ("éŸ©è¯­ | Korean", "ko"),
                            ("ç²¤è¯­ | Cantonese", "yue"),
                        ],
                        value="auto",
                    )
                    btn_stt = gr.Button(
                        "å¼€å§‹è¯†åˆ« | Start Recognition", variant="primary"
                    )

                with gr.Column():
                    text_output = gr.Textbox(
                        label="è¯†åˆ«ç»“æœ | Recognition Result", lines=5
                    )
                    lang_output = gr.Textbox(
                        label="è¯­è¨€ä¿¡æ¯ | Language Info", interactive=False
                    )

            btn_stt.click(
                speech_to_text,
                inputs=[audio_input, language],
                outputs=[text_output, lang_output],
            )

        # Tab 2: è¯­éŸ³åˆæˆ | Text to Speech
        with gr.Tab("è¯­éŸ³åˆæˆ | Text to Speech"):
            gr.Markdown("### ğŸ”Š è¯­éŸ³åˆæˆ | Text to Speech")
            with gr.Row():
                with gr.Column():
                    text_input = gr.Textbox(
                        label="è¾“å…¥æ–‡æœ¬ | Input Text",
                        lines=3,
                        placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬... | Enter text to synthesize...",
                    )
                    voice_select = gr.Dropdown(
                        label="é€‰æ‹©éŸ³è‰² | Select Voice",
                        choices=voices,
                        value=voices[0] if voices else "ä¸­æ–‡å¥³",
                    )
                    btn_tts = gr.Button("ç”Ÿæˆè¯­éŸ³ | Generate Speech", variant="primary")

                with gr.Column():
                    audio_output = gr.Audio(
                        label="ç”Ÿæˆçš„è¯­éŸ³ | Generated Speech", type="filepath"
                    )
                    status_output = gr.Textbox(label="çŠ¶æ€ | Status", interactive=False)

            btn_tts.click(
                text_to_speech,
                inputs=[text_input, voice_select],
                outputs=[audio_output, status_output],
            )

        # Tab 3: AIå¯¹è¯ | AI Chat
        with gr.Tab("AIå¯¹è¯ | AI Chat"):
            gr.Markdown("### ğŸ¤– AIå¯¹è¯ | AI Chat")
            gr.Markdown(
                "ä¸æœ¬åœ°å¤§æ¨¡å‹å¯¹è¯ | Chat with local LLMï¼ˆæ”¯æŒå›¾ç‰‡ä¸Šä¼ å’Œå¤šè½®è®°å¿† | Supports image upload and multi-turn memoryï¼‰"
            )

            # è®¾ç½®åŒºåŸŸ
            with gr.Accordion(
                "âš™ï¸ å¯¹è¯è®¾ç½® | Chat Settingsï¼ˆç‚¹å‡»å±•å¼€ | Click to expandï¼‰", open=False
            ):
                gr.Markdown(
                    "è°ƒæ•´AIå›å¤é•¿åº¦ã€è®°å¿†ç®¡ç†å’Œå…¶ä»–å‚æ•° | Adjust AI response length, memory management and other parameters"
                )
                with gr.Row():
                    with gr.Column():
                        max_tokens_input = gr.Number(
                            label="Token é™åˆ¶ | Token Limitï¼ˆå›å¤æœ€å¤§å­—æ•° | Max response lengthï¼‰",
                            value=current_max_tokens,
                            minimum=30,
                            maximum=500,
                            step=10,
                            info="æ•°å€¼è¶Šå°å›å¤è¶ŠçŸ­ï¼Œå»ºè®®80-150 | Smaller value = shorter response, recommended 80-150",
                        )
                        max_history_input = gr.Number(
                            label="æœ€å¤§è®°å¿†è½®æ•° | Max Memory Rounds",
                            value=current_max_history,
                            minimum=1,
                            maximum=20,
                            step=1,
                            info="ä¿ç•™æœ€è¿‘Nè½®å¯¹è¯ | Keep last N conversation rounds",
                        )
                    with gr.Column():
                        system_prompt_input = gr.Textbox(
                            label="AI è¡Œä¸ºè®¾å®š | AI Behaviorï¼ˆSystem Promptï¼‰",
                            value=current_system_prompt,
                            lines=3,
                            info="å®šä¹‰AIçš„å›å¤é£æ ¼ | Define AI response style",
                        )
                with gr.Row():
                    save_btn = gr.Button(
                        "ğŸ’¾ ä¿å­˜è®¾ç½® | Save Settings", variant="secondary"
                    )
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè®°å¿† | Clear Memory", variant="stop")
                settings_status = gr.Textbox(label="çŠ¶æ€ | Status", interactive=False)
                memory_info = gr.Textbox(
                    label="è®°å¿†çŠ¶æ€ | Memory Status",
                    value=get_memory_info(initial_session_id),
                    interactive=False,
                )

                # ç»‘å®šæŒ‰é’®äº‹ä»¶ï¼ˆclearäº‹ä»¶åœ¨chatbotå®šä¹‰åç»‘å®šï¼‰
                save_btn.click(
                    update_settings,
                    inputs=[max_tokens_input, system_prompt_input, max_history_input],
                    outputs=settings_status,
                )

            with gr.Row():
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯è®°å½• | Chat History",
                        height=400,
                        value=chat_memory.get_display_history(initial_session_id),
                    )
                    # ç»‘å®šæ¸…ç©ºæŒ‰é’®äº‹ä»¶ï¼ˆåœ¨chatbotå®šä¹‰åï¼‰
                    clear_btn.click(
                        clear_memory,
                        inputs=[session_id_state],
                        outputs=[settings_status, chatbot],
                    )
                    msg_input = gr.Textbox(
                        label="è¾“å…¥æ¶ˆæ¯ | Input Message",
                        placeholder="è¾“å…¥æ¶ˆæ¯æŒ‰å›è½¦å‘é€... | Enter message and press Enter...",
                    )
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰| Upload Image (Optional)",
                        type="filepath",
                        height=300,
                    )
                    gr.Markdown("""
                    **ä½¿ç”¨è¯´æ˜ | Usage:**
                    - ğŸ’¬ ä»…æ–‡å­— | Text only: ç›´æ¥è¾“å…¥æ¶ˆæ¯ | Type message directly
                    - ğŸ“· å›¾æ–‡å¯¹è¯ | Image + Text: ä¸Šä¼ å›¾ç‰‡ + è¾“å…¥é—®é¢˜ | Upload image + type question
                    - ğŸ”„ è®°å¿†åŠŸèƒ½ | Memory: è‡ªåŠ¨ä¿ç•™ä¸Šä¸‹æ–‡ | Auto-save conversation context
                    - ğŸ—‘ï¸ æ¸…ç©ºè®°å¿† | Clear: åœ¨è®¾ç½®ä¸­ç‚¹å‡»æ¸…ç©º | Click Clear Memory in settings
                    """)

            def respond(message, image, history, session_id):
                if not message.strip() and image is None:
                    return "", history, get_memory_info(session_id)

                # è°ƒç”¨AIï¼ˆè‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†ï¼‰
                response = chat_with_ai(session_id, message, image)

                # æ›´æ–°æ˜¾ç¤º
                new_history = chat_memory.get_display_history(session_id)

                return "", new_history, get_memory_info(session_id)

            msg_input.submit(
                respond,
                inputs=[msg_input, image_input, chatbot, session_id_state],
                outputs=[msg_input, chatbot, memory_info],
            )

        # Tab 4: å®Œæ•´æµç¨‹ | Full Pipeline
        with gr.Tab("å®Œæ•´æµç¨‹ | Full Pipeline"):
            gr.Markdown("### ğŸ”„ å®Œæ•´æµç¨‹ | Full Pipeline")
            gr.Markdown(
                "è¯­éŸ³/æ–‡å­— â†’ AIç†è§£ â†’ è¯­éŸ³å›å¤ | Voice/Text â†’ AI Understanding â†’ Voice Response"
            )

            with gr.Row():
                with gr.Column():
                    complete_audio_input = gr.Audio(
                        label="ğŸ¤ ä¸Šä¼ è¯­éŸ³ | Upload Voiceï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ | Optional, priorityï¼‰",
                        type="filepath",
                    )
                    complete_text_input = gr.Textbox(
                        label="âœï¸ æˆ–ç›´æ¥è¾“å…¥æ–‡å­— | Or type text",
                        placeholder="å¦‚æœä¸ä¸Šä¼ è¯­éŸ³ï¼Œè¯·åœ¨è¿™é‡Œè¾“å…¥æ–‡å­—... | If no voice, type here...",
                        lines=2,
                    )
                    complete_image_input = gr.Image(
                        label="ğŸ“· ä¸Šä¼ å›¾ç‰‡ | Upload Imageï¼ˆå¯é€‰ | Optionalï¼‰",
                        type="filepath",
                        height=200,
                    )
                    complete_voice = gr.Dropdown(
                        label="é€‰æ‹©å›å¤éŸ³è‰² | Select Response Voice",
                        choices=voices,
                        value=voices[0] if voices else "ä¸­æ–‡å¥³",
                    )
                    btn_complete = gr.Button(
                        "å¼€å§‹å¯¹è¯ | Start Conversation", variant="primary"
                    )

                with gr.Column():
                    complete_audio_output = gr.Audio(
                        label="AIå›å¤è¯­éŸ³ | AI Response Voice", type="filepath"
                    )
                    complete_text_output = gr.Textbox(
                        label="AIå›å¤æ–‡æœ¬ | AI Response Text", lines=2
                    )
                    complete_asr_output = gr.Textbox(
                        label="è¾“å…¥å†…å®¹ | Input Contentï¼ˆè¯­éŸ³è¯†åˆ«çš„æ–‡å­—æˆ–æ‚¨è¾“å…¥çš„æ–‡å­— | Voice recognition or your textï¼‰",
                        lines=2,
                    )

            btn_complete.click(
                complete_pipeline,
                inputs=[
                    session_id_state,
                    complete_audio_input,
                    complete_text_input,
                    complete_image_input,
                    complete_voice,
                ],
                outputs=[
                    complete_audio_output,
                    complete_text_output,
                    complete_asr_output,
                ],
            )

    gr.Markdown(f"""
    ---
    **VoiceForge** v1.0.0-preview | ä¼šè¯ID | Session ID: `{initial_session_id}` | æ”¯æŒå¤šè½®è®°å¿† | Multi-turn Memory Supported
    """)

# ==================== å¯åŠ¨æœåŠ¡ ====================

if __name__ == "__main__":
    port = web_config.get("port", 7860)
    share = web_config.get("share", False)

    print(f"\nğŸŒ å¯åŠ¨ Web ç•Œé¢ | Starting Web Interface...")
    print(f"   åœ°å€ | Address: http://localhost:{port}")
    print(f"   ä¼šè¯ID | Session ID: {initial_session_id}")
    print(
        f"   è®°å¿†ç®¡ç† | Memory Management: å·²å¯ç”¨ | Enabledï¼ˆæœ€å¤§{chat_memory.max_history}è½® | max {chat_memory.max_history} roundsï¼‰"
    )
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡ | Press Ctrl+C to stop\n")

    demo.launch(server_name="0.0.0.0", server_port=port, share=share)
